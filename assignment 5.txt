



package Asiignment5;

import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


import java.io.IOException;
import java.util.logging.Logger;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.TextInputFormat;

public class SumShared {
	public static  int Total1 =0;
	
	public static class SumSharedMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
		
		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException{
	
		    if(recordIsInvalid(value)==false & value.toString().split("\\|")[3].equals("1")){
		        Text TrackId1 = new Text();
			IntWritable Share = new IntWritable();				
			TrackId1 = new Text(value.toString().split("\\|")[1]);
			Share = new IntWritable(1);
			context.write(TrackId1, Share );
		    }

		}
		
		private boolean recordIsInvalid(Text record){
			
			String[] lineArray = record.toString().split("\\|");
			boolean isInvalid = false;
	        for(int i=0;i<lineArray.length;i++){
	        	if(lineArray[i].equals("NA")){
	        		isInvalid = true;
	        	}
	        }
	        return isInvalid;
		}
	}
	public static class SumSharedReducer extends Reducer<Text, IntWritable, String, IntWritable>{
		
		private IntWritable result = new IntWritable();
		private String Shared = "Total Shared listens";
		
		public void reduce (Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException{			
			
			for(IntWritable val: values){
				Total1 += val.get();
				
			}
			result.set(Total1);
			context.write(Shared, result);
		}
	}

	public static void main(String[] args) throws Exception{

	    Configuration conf = new Configuration();	
		
	    Job job = new Job(conf, "Share listeners per track");
		job.setJarByClass(Sum.class);
		job.setMapperClass(SumSharedMapper.class);
		job.setReducerClass(SumSharedReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	    
	    System.exit(job.waitForCompletion(true) ? 0 : 1);  	    	    	    
	}
}
















public void map(LongWritable position, Text rawLine, OutputCollector<IntWritable, TrackStats> output, Reporter reporter) throws IOException {

String[] parts = (rawLine.toString()).split(" ");

int trackId = Integer.parseInt(parts[TrackStatisticsProgram.COL_TRACKID]);

int Share = Integer.parseInt(parts[TrackStatisticsProgram.COL_SHARE]);
 int Platform = Integer.parseInt(parts[TrackStatisticsProgram.COL_PLATFORM]);

int skip = Integer.parseInt(parts[TrackStatisticsProgram.COL_SKIP]);

//set number of listeners to 0 (this is calculated later)

//and other values as provided in text file

TrackStats trackstat = new TrackStats(0, share + platform, share, platform, skip);
 output.collect(new IntWritable(trackId), trackstat);

}



public void reduce(IntWritable trackId, Iterator<TrackStats> values, OutputCollector<IntWritable, TrackStats> output, Reporter reporter) throws IOException {

TrackStats sum = new TrackStats(); // holds the totals for this track
 while (values.hasNext()) {

TrackStats trackStats = (TrackStats) values.next();
 sum.setListeners(sum.getListeners() + trackStats.getListeners());
 sum.setPlays(sum.getPlays() + trackStats.getPlays()); 
sum.setSkips(sum.getSkips() + trackStats.getSkips()); 
sum.setScrobbles(sum.getScrobbles() + trackStats.getShare()); 
sum.setRadioPlays(sum.getRadioPlays() + trackStats.getRadioPlays());

}

output.collect(trackId, sum);

}
























package Asiignment5;

import java.io.IOException;
import java.util.HashSet;
import java.util.Set;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
public class UniqueListeners {
	private enum COUNTERS {
		INVALID_RECORD_COUNT
	}

	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		if (args.length != 2) {
			System.err.println("Usage: uniquelisteners <in> <out>");
			System.exit(2);
		}
		Job job = new Job(conf, "Unique listeners per track");
		job.setJarByClass(UniqueListeners.class);
		job.setMapperClass(UniqueListenersMapper.class);
		job.setReducerClass(UniqueListenersReducer.class);
		job.setOutputKeyClass(IntWritable.class);
		job.setOutputValueClass(IntWritable.class);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
		org.apache.hadoop.mapreduce.Counters counters = job.getCounters();
		System.out.println("No. of Invalid Records :"
				+ counters.findCounter(COUNTERS.INVALID_RECORD_COUNT)
						.getValue());
	}

	public static class UniqueListenersReducer extends
			Reducer<IntWritable, IntWritable, IntWritable, IntWritable> {

		public void reduce(
				IntWritable trackId,
				Iterable<IntWritable> userIds,
				Reducer<IntWritable, IntWritable, IntWritable, IntWritable>.Context context)
				throws IOException, InterruptedException {

			Set<Integer> userIdSet = new HashSet<Integer>();
			for (IntWritable userId : userIds) {
				userIdSet.add(userId.get());
			}
			IntWritable size = new IntWritable(userIdSet.size());
			context.write(trackId, size);
		}
	}

	public static class UniqueListenersMapper extends
			Mapper<Object, Text, IntWritable, IntWritable> {

		IntWritable trackId = new IntWritable();
		IntWritable userId = new IntWritable();

		public void map(Object key, Text value,
				Mapper<Object, Text, IntWritable, IntWritable>.Context context)
				throws IOException, InterruptedException {

			String[] parts = value.toString().split("[|]");
			trackId.set(Integer.parseInt(parts[MusicData.TRACK_ID]));
			userId.set(Integer.parseInt(parts[MusicData.USER_ID]));

			if (parts.length == 5) {
				context.write(trackId, userId);
			} else {
				// add counter for invalid records
				context.getCounter(COUNTERS.INVALID_RECORD_COUNT).increment(1L);
			}

		}
	}
}





package Asiignment5;

public class MusicData {

	public static final int USER_ID = 0;
	public static final int TRACK_ID = 1;
	public static final int Share = 2;
	public static final int Platform = 3;
	public static final int Listening_Status = 4;




package Asiignment5;

import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


import java.io.IOException;
import java.util.logging.Logger;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.TextInputFormat;

public class Sum {
	public static  int Total =0;
	
	public static class SumMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
		
		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException{
	
		    if(recordIsInvalid(value)==false & value.toString().split("\\|")[4].equals("1")){
		        Text TrackId1 = new Text();
			IntWritable full = new IntWritable();				
			TrackId1 = new Text(value.toString().split("\\|")[1]);
			full = new IntWritable(1);
			context.write(TrackId1, full );
		    }

		}
		
		private boolean recordIsInvalid(Text record){
			
			String[] lineArray = record.toString().split("\\|");
			boolean isInvalid = false;
	        for(int i=0;i<lineArray.length;i++){
	        	if(lineArray[i].equals("NA")){
	        		isInvalid = true;
	        	}
	        }
	        return isInvalid;
		}
	}
	public static class SumReducer extends Reducer<Text, IntWritable, String, IntWritable>{
		
		private IntWritable result = new IntWritable();
		private String Full = "Tota full listens";
		
		public void reduce (Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException{			
			
			for(IntWritable val: values){
				Total += val.get();
				
			}
			result.set(Total);
			context.write(Full, result);
		}
	}

	public static void main(String[] args) throws Exception{

	    Configuration conf = new Configuration();	
		
	    Job job = new Job(conf, "Full listeners per track");
		job.setJarByClass(Sum.class);
		job.setMapperClass(SumMapper.class);
		job.setReducerClass(SumReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	    
	    System.exit(job.waitForCompletion(true) ? 0 : 1);  	    	    	    
	}
}

	
}
